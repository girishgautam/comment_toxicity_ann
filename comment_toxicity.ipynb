{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46616840",
   "metadata": {},
   "source": [
    "# Install Dependencies and Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb08050",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: pandas in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (1.4.4)\n",
      "Requirement already satisfied: matplotlib in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.9.24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4585f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:52:18.603176: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 11:52:19.565030: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-13 11:52:19.565097: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-06-13 11:52:19.673090: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-13 11:52:22.001579: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-13 11:52:22.001733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-13 11:52:22.001744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece50c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is downloaded from kaggle - kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "df = pd.read_csv(os.path.join('data', 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07cddc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903dd13",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdc08670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2c8a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the text before vectorising\n",
    "def clean(text):\n",
    "    for puncuation in string.punctuation:\n",
    "        text = text.replace(puncuation, ' ')\n",
    "    \n",
    "    lowercase = text.lower()\n",
    "    tokenised = word_tokenize(lowercase)\n",
    "    word_only = [word for word in tokenised if word.isalpha()]\n",
    "    \n",
    "    stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "    without_stopword = [word for word in word_only if word not in stop_words]\n",
    "    \n",
    "    verb_lemmatized = [ WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in without_stopword ]\n",
    "    \n",
    "    noun_lemmatised = [ WordNetLemmatizer().lemmatize(word, pos='n')\n",
    "                      for word in verb_lemmatized ]\n",
    "    \n",
    "    return \" \".join(noun_lemmatised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c1ab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment_text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "389e8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6fec576",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb18e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 14:12:20.879413: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-06-13 14:12:20.894599: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-06-13 14:12:20.894660: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-T6TOAJH): /proc/driver/nvidia/version does not exist\n",
      "2024-06-13 14:12:20.896830: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TextVectorization(max_tokens=max_words,\n",
    "                               output_sequence_length=1000,\n",
    "                               output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fab01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f17ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 14:19:12.833051: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1276568000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5182610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCSHBAP - map, chache, shuffle, batch, prefetch  from_tensor_slices, list_file\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(16)\n",
    "dataset = dataset.prefetch(8) # helps bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0af61a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 14:28:23.763994: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1276568000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5540815",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1)+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395005b6",
   "metadata": {},
   "source": [
    "# Create Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "128b9513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63b51f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Create the embedding layer \n",
    "model.add(Embedding(max_words+1, 32))\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer \n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31ab4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e82ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          3200032   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               16640     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,291,686\n",
      "Trainable params: 3,291,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ce2682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6981/6981 [==============================] - ETA: 0s - loss: 0.0572"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:51:31.925052: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1276568000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "6981/6981 [==============================] - 3841s 550ms/step - loss: 0.0572 - val_loss: 0.0462\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=1, validation_data=val, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4824c21",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fc757f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = vectorizer(clean('You freaking idiot! I am going to kill you fucking moron.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "432c4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(np.expand_dims(input_text, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "10b8b019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999438 , 0.4886239 , 0.99360424, 0.0170372 , 0.9486099 ,\n",
       "        0.1633346 ]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c1db4d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee2bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
